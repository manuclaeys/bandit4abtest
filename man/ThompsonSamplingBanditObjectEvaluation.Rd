% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/thompson_sampling_bandit_object_evaluation.R
\name{ThompsonSamplingBanditObjectEvaluation}
\alias{ThompsonSamplingBanditObjectEvaluation}
\title{ThompsonSamplingBanditObjectEvaluation}
\usage{
ThompsonSamplingBanditObjectEvaluation(
  visitor_reward = visitor_reward,
  K = ncol(visitor_reward),
  alpha = 1,
  beta = 1,
  average = FALSE,
  IsRewardAreBoolean = TRUE,
  dt = NA,
  explanatory_variable = colnames(dt)
)
}
\arguments{
\item{visitor_reward}{Dataframe of integer or numeric values}

\item{K}{Integer value (optional)}

\item{alpha}{Numeric value (optional)}

\item{beta}{Numeric value (optional)}

\item{average}{Boolean value (optional)}
}
\value{
\itemize{ List of element:
 \item ThompsonSampling_alloc: ThompsonSampling object ,
 \item cum_reg_ucb_alloc: List numeric.
 }
}
\description{
Run the Thompson Sampling algorithm using visitor_reward values with \code{\link{ThompsonSampling}} function.
Stop if something is wrong.
After execution of Thompson Sampling, calculates the cumulative regret
associated with the choices maded.
Review the cumulative regret according iterations and an thompson sampling object.
See also \code{\link{ThompsonSampling}}, \code{\link{CumulativeRegret}}
Require \code{\link{tic}} and \code{\link{toc}} from \code{\link{tictoc}} library.
Average option implies a regret based on conditional inference tree model.
}
\examples{
## Generates 1000 numbers from 2 binomial distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame(cbind(K1,K2) )
#Run Thompson Sampling algorithm with policy evaluation
ThompsonSamplingBanditObjectEvaluation(visitor_reward,alpha = 1, beta = 1 )
ThompsonSamplingBanditObjectEvaluation(visitor_reward= visitor_reward,alpha = 1, beta = 1 ,average = TRUE,IsRewardAreBoolean = TRUE)
}
