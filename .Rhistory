#'visitor_reward <- as.data.frame(cbind(K1,K2) )
#'#remove data
#'temp_list <- sample(1:nrow(visitor_reward), 500, replace = FALSE, prob = NULL)
#'visitor_reward$K1[temp_list] <- NA
#'visitor_reward$K2[-temp_list] <- NA
#'#run ucb on missing data
#'ucb_alloc  <- UCB_rejection_sampling(visitor_reward,alpha = 10)
#'@import tictoc
#'@export
UCB_rejection_sampling <- function(visitorReward, K=ncol(visitorReward) , alpha = 1){
#data formating
visitorReward <- as.matrix(visitorReward)
#keep list of choice
#keep list of choice
choice <- c()
proba <- c()
rewards <- c()
S <- GenerateMatrixS(K)
tic()
#tempo variables
temp_i=1
i=1
###initialisation
while(temp_i<(K+1)){
####Rejection sampling
### le bon choix
if(is.na(visitorReward[i,temp_i])==FALSE){
choice[temp_i] =  temp_i
###Si choix réel
# see what kind of result we get
rewards[temp_i] = visitorReward[i,temp_i]
# update the input vector
S <- PlayArm(iter=i,arm=temp_i,S,visitorReward)
proba[temp_i] <-  max(ProbaMaxForUCB(S=S, iter=temp_i, alpha=alpha, K))
temp_i = temp_i +1
}
i=i+1
}
for(i in i:nrow(visitorReward)){
#    message(S)
choice[i] <- ConditionForUCB(S,iter=temp_i,alpha=alpha,K)
####Rejection sampling
### le bon choix
if(is.na(visitorReward[i,as.integer(choice[i])])==FALSE){
###Si choix réel
# see what kind of result we get
rewards[temp_i] = visitorReward[i,as.integer(choice[i])]
# update the input vector
S <- PlayArm(iter=i,arm=choice[i],S,visitorReward)
proba[i] <-  max(ProbaMaxForUCB(S=S, iter=i, alpha=alpha, K))
temp_i = temp_i +1
}else{
choice[i]= NA
}
}
time <- toc()
options(scipen=999)
#coef estimate
th_hat=S[1,]
#real coef
th = colMeans(visitorReward,na.rm =TRUE)
message("th_hat")
message(th_hat)
message("th real")
message(th)
message(paste('number of used items', sum(S[2,1],S[2,2])),', number of excluded items :',(nrow(visitorReward) - sum(S[2,1],S[2,2])), sep= " " )
return (list('S'=S,'choice'= choice,'proba' = proba,'time'=(time$toc - time$tic),'theta_hat'=th_hat,'theta'=th))
}
ucb_alloc  <- UCB_rejection_sampling(visitorReward, alpha = 1)
ucb_alloc$choice
#'UCB_rejection_sampling
#'
#'UCB algorithme with rejection sampling method.
#'Exclud any choices which not corresponds to real exepriments in dataset
#'Stop if something is wrong.
#'Generate a matrix to save the results (S).
#' \itemize{ At each iteration,
#'  \item Calculates the arm probabilities,
#'  \item Choose the arm with the maximum upper bound (with alpha parameter)
#'  \item Receives a reward in visitor_reward for the arm and associated iteration
#'  \item Updates the results matrix S.
#'  }
#'Returns the calculation time.
#'Review the estimated, actual averages and number of choices for each arm.
#'See also \code{\link{ConditionForUCB}}, \code{\link{GenerateMatrixS}},
#'\code{\link{ProbaMaxForUCB}} and \code{\link{PlayArm}}.
#'Require \code{\link{tic}} and \code{\link{toc}} from \code{\link{tictoc}} library
#'@param visitor_reward Dataframe of integer or numeric values
#'@param K Integer value (optional)
#'@param alpha Numeric value (optional)
#'
#'@return
#' \itemize{ List of element:
#'  \item S:numerical matrix of results ,
#'  \item choice: choices of UCB,
#'  \item proba: probability of the chosen arms,
#'  \item time: time of cumputation,
#'  \item theta_hat: mean estimated of each arm
#'  \item theta: real mean of each arm
#'  }
#'
#'@examples
#'## Generates 10000 numbers from 2 binomial  distributions
#'set.seed(4434)
#'K1 <- rbinom(1000, 1, 0.6)
#'K2 <- rbinom(1000, 1, 0.7)
#'## Define a dataframe of rewards
#'visitor_reward <- as.data.frame(cbind(K1,K2) )
#'#remove data
#'temp_list <- sample(1:nrow(visitor_reward), 500, replace = FALSE, prob = NULL)
#'visitor_reward$K1[temp_list] <- NA
#'visitor_reward$K2[-temp_list] <- NA
#'#run ucb on missing data
#'ucb_alloc  <- UCB_rejection_sampling(visitor_reward,alpha = 10)
#'@import tictoc
#'@export
UCB_rejection_sampling <- function(visitorReward, K=ncol(visitorReward) , alpha = 1){
#data formating
visitorReward <- as.matrix(visitorReward)
#keep list of choice
#keep list of choice
choice <- c()
proba <- c()
rewards <- c()
S <- GenerateMatrixS(K)
tic()
#tempo variables
temp_i=1
i=1
###initialisation
while(temp_i<(K+1)){
####Rejection sampling
### le bon choix
if(is.na(visitorReward[i,temp_i])==FALSE){
choice[i] =  temp_i
###Si choix réel
# see what kind of result we get
rewards[temp_i] = visitorReward[i,temp_i]
# update the input vector
S <- PlayArm(iter=i,arm=temp_i,S,visitorReward)
proba[temp_i] <-  max(ProbaMaxForUCB(S=S, iter=temp_i, alpha=alpha, K))
temp_i = temp_i +1
}else{
choice[i] = NA
}
i=i+1
}
for(i in i:nrow(visitorReward)){
#    message(S)
choice[i] <- ConditionForUCB(S,iter=temp_i,alpha=alpha,K)
####Rejection sampling
### le bon choix
if(is.na(visitorReward[i,as.integer(choice[i])])==FALSE){
###Si choix réel
# see what kind of result we get
rewards[temp_i] = visitorReward[i,as.integer(choice[i])]
# update the input vector
S <- PlayArm(iter=i,arm=choice[i],S,visitorReward)
proba[i] <-  max(ProbaMaxForUCB(S=S, iter=i, alpha=alpha, K))
temp_i = temp_i +1
}else{
choice[i]= NA
}
}
time <- toc()
options(scipen=999)
#coef estimate
th_hat=S[1,]
#real coef
th = colMeans(visitorReward,na.rm =TRUE)
message("th_hat")
message(th_hat)
message("th real")
message(th)
message(paste('number of used items', sum(S[2,1],S[2,2])),', number of excluded items :',(nrow(visitorReward) - sum(S[2,1],S[2,2])), sep= " " )
return (list('S'=S,'choice'= choice,'proba' = proba,'time'=(time$toc - time$tic),'theta_hat'=th_hat,'theta'=th))
}
ucb_alloc  <- UCB_rejection_sampling(visitorReward, alpha = 1)
ucb_alloc$choice
## Generates 10000 numbers from 2 binomial  distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame(cbind(K1,K2) )
#remove data
temp_list <- sample(1:nrow(visitor_reward), 500, replace = FALSE, prob = NULL)
visitor_reward$K1[temp_list] <- NA
visitor_reward$K2[-temp_list] <- NA
#run ucb on missing data
ucb_alloc  <- UCB_rejection_sampling(visitor_reward,alpha = 10)
ucb_rejection_sampling_alloc  <- UCB_rejection_sampling(visitor_reward,alpha = 10)
K=ncol(visitor_reward)
alpha=1
ucb_rejection_sampling_bandit_alloc <- UCB_rejection_sampling(visitor_reward, alpha = alpha, K=K)
cum_rew_ucb_rejection_sampling_alloc <- reward_cumulative(choice=choice,visitor_reward=visitor_reward)
cum_rew_ucb_rejection_sampling_alloc <- reward_cumulative(choice=ucb_rejection_sampling_bandit_alloc$choice,visitor_reward=visitor_reward)
ucb_rejection_sampling_bandit_alloc <- UCB_rejection_sampling(visitor_reward, alpha = alpha, K=K)
cum_rew_ucb_rejection_sampling_alloc <- reward_cumulative(choice=ucb_rejection_sampling_bandit_alloc$choice,visitor_reward=visitor_reward)
anyNA(ucb_rejection_sampling_bandit_alloc$choice)
#'Return list of cumulative reward
#'
#'Return a list with cumulative rewards at each iterations. Can be used for rejection samplig method.
#'
#'@param choice  Integer list
#'@param visitor_reward dataframe of integer or numeric values
#'
#'@return List of numeric values
#'
#'@examples
#'##### Pairewise #####
#'set.seed(1234)
#'size.tot <- 10000
#'x <- seq(0, 5, 0.01)
#'x1<- sample(x, size.tot, replace = TRUE, prob = NULL)
#'arm_1 <-  as.vector(c(2,-1,1.5,0))
#'K1 <- (x1 < 1 ) * arm_1[4]  +
#'  (x1 >= 1 & x1 < 2 ) * arm_1[1]  +
#'  (x1 >= 2 & x1 < 3) * arm_1[2]  +
#'  (x1 >= 3 & x1 < 4) * arm_1[3]  +
#'  (x1 >= 4) * arm_1[4]
#'plot(x1, K1)
#'
#'arm_2 <-  as.vector(c(1.5,-0.5,1.25,0))
#'K2 <- (x1 < 1 ) * arm_2[4]  +
#'  (x1 >= 1 & x1 < 2 ) * arm_2[1]  +
#'  (x1 >= 2 & x1 < 3) * arm_2[2]  +
#'  (x1 >= 3 & x1 < 4) * arm_2[3]  +
#'  (x1 >= 4) * arm_2[4]
#'plot(x1, K2)
#'#covariate without interest
#'x2<- sample(x, size.tot, replace = TRUE, prob = NULL)
#'#Results for each variation
#'visitor_reward <-  data.frame(K1,K2 )
#'summary(visitor_reward)
#'dt <- as.data.frame(cbind(x1,x2))
#'#Random choices
#'choice <- sample(c(1,2), size.tot, replace = TRUE)
#'reward <- reward_cumulative(choice=choice,visitor_reward=visitor_reward)
#'plot(1:size.tot,  cumsum(reward))
#'@export
reward_cumulative <- function(choice, visitor_reward){
if(anyNA(choice)==FALSE){
reward.evolutive <- c()
for(i in 1:nrow(visitor_reward)) reward.evolutive[i] <- visitor_reward[i,choice[i]]
return(cumsum(reward.evolutive))
}else{
reward.evolutive <- c()
j=1
for(i in 1:nrow(visitor_reward)){
if(is.na(choice[i])==FALSE){
reward.evolutive[j] <- visitor_reward[i,choice[i]]
j=j+1
}
}
return(cumsum(reward.evolutive))
}
}
cum_rew_ucb_rejection_sampling_alloc <- reward_cumulative(choice=ucb_rejection_sampling_bandit_alloc$choice,
visitor_reward=visitor_reward)
if(average == TRUE) cum_reg_ucb_bandit_alloc <- cumulativeRegretAverage(ucb_bandit_alloc$choice,
visitor_reward = visitor_reward,
dt=dt.reward,
IsRewardAreBoolean=IsRewardAreBoolean,
explanatory_variable=explanatory_variable)
cum_rew_ucb_rejection_sampling_alloc <- reward_cumulative(choice=ucb_rejection_sampling_bandit_alloc$choice,
visitor_reward=visitor_reward)
cum_rew_ucb_rejection_sampling_alloc <- reward_cumulative(choice=ucb_rejection_sampling_bandit_alloc$choice,
visitor_reward=visitor_reward)
cum_rew_ucb_rejection_sampling_alloc
plot(cum_rew_ucb_rejection_sampling_alloc)
plot(cum_rew_ucb_rejection_sampling_alloc, type = 'l')
max(cum_rew_ucb_rejection_sampling_alloc)/length(cum_rew_ucb_rejection_sampling_alloc)
message(paste("average : "),max(cum_rew_ucb_rejection_sampling_alloc)/length(cum_rew_ucb_rejection_sampling_alloc), sep = " " )
library(bandit4abtest)
?plot
plot(cum_rew_ucb_rejection_sampling_alloc, type = 'l',xlab = "Ucb Rejection Sampling")
plot(cum_rew_ucb_rejection_sampling_alloc, type = 'l',ylab = "Ucb Rejection Sampling")
#################
rm(list = ls())
#Contextual with continus reward
size.tot = 10000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 2, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2)
#arm reward
arm_1 <-  as.vector(c(0,0.5))
K1 = crossprod(t(dt),arm_1) + runif(size.tot, min=-1, max=1)    #  linear predictor
summary(K1)
arm_2 <-  as.vector(c(0.5,0))
K2 = crossprod(t(dt),arm_2) + runif(size.tot, min=-1, max=1)
summary(K2)
visitor_reward <-  data.frame(K1,K2)
dt <- as.data.frame(dt)
linucb_contextual_alloc <- LINUCB(dt,visitor_reward )
cum_reg_linucb_contextual_alloc <- cumulativeRegretAverage(linucb_contextual_alloc$choice,visitor_reward,dt = dt)
K1 = crossprod(t(dt),arm_1)     #  linear predictor
summary(K1)
arm_2 <-  as.vector(c(0.5,0))
K2 = crossprod(t(dt),arm_2)
summary(K2)
visitor_reward <-  data.frame(K1,K2)
dt <- as.data.frame(dt)
linucb_contextual_alloc <- LINUCB(dt,visitor_reward )
cum_reg_linucb_contextual_alloc <- cumulativeRegretAverage(linucb_contextual_alloc$choice,visitor_reward,dt = dt)
?UCB
?EpsilonGreedy
library(bandit4abtest)
size.tot = 2000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
x3 = runif(size.tot, min=0, max=10)
x4 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2,x3,x4)
#arm reward
arm_1 <-  as.vector(c(-1,9,-8,4))
K1 = crossprod(t(dt),arm_1)
arm_2 <-  as.vector(c(-1,2,1,0))
K2 = crossprod(t(dt),arm_2)
arm_3 <-  as.vector(c(-1,-5,1,10))
K3 = crossprod(t(dt),arm_3)
visitor_reward <-  data.frame(K1,K2,K3)
dt <- as.data.frame(dt)
thompson_sampling_contextual_alloc <- TSLINUCB(dt,visitor_reward, iter = 1000)
cum_reg_thompson_sampling_contextual_alloc <- cumulativeRegretAverage(thompson_sampling_contextual_alloc$choice,visitor_reward,dt = dt)
library(bandit4abtest)
size.tot = 2000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
x3 = runif(size.tot, min=0, max=10)
x4 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2,x3,x4)
#arm reward
arm_1 <-  as.vector(c(-1,9,-8,4))
K1 = crossprod(t(dt),arm_1)
arm_2 <-  as.vector(c(-1,2,1,0))
K2 = crossprod(t(dt),arm_2)
arm_3 <-  as.vector(c(-1,-5,1,10))
K3 = crossprod(t(dt),arm_3)
visitor_reward <-  data.frame(K1,K2,K3)
dt <- as.data.frame(dt)
thompson_sampling_contextual_alloc <- TSLINUCB(dt,visitor_reward, iter = 1000)
cum_reg_thompson_sampling_contextual_alloc <- cumulativeRegretAverage(thompson_sampling_contextual_alloc$choice,visitor_reward,dt = dt,message_tree = TRUE)
size.tot = 2000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
x3 = runif(size.tot, min=0, max=10)
x4 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2,x3,x4)
#arm reward
arm_1 <-  as.vector(c(-1,9,-8,4))
K1 = crossprod(t(dt),arm_1)
arm_2 <-  as.vector(c(-1,2,1,0))
K2 = crossprod(t(dt),arm_2)
arm_3 <-  as.vector(c(-1,-5,1,10))
K3 = crossprod(t(dt),arm_3)
visitor_reward <-  data.frame(K1,K2,K3)
dt <- as.data.frame(dt)
thompson_sampling_contextual_alloc <- TSLINUCB(dt,visitor_reward, iter = 1000)
cum_reg_thompson_sampling_contextual_alloc <- cumulativeRegretAverage(thompson_sampling_contextual_alloc$choice,visitor_reward,dt = dt,message_tree = TRUE)
size.tot = 2000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
x3 = runif(size.tot, min=0, max=10)
x4 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2,x3,x4)
#arm reward
arm_1 <-  as.vector(c(-1,9,-8,4))
K1 = crossprod(t(dt),arm_1)
arm_2 <-  as.vector(c(-1,2,1,0))
K2 = crossprod(t(dt),arm_2)
arm_3 <-  as.vector(c(-1,-5,1,10))
K3 = crossprod(t(dt),arm_3)
visitor_reward <-  data.frame(K1,K2,K3)
dt <- as.data.frame(dt)
thompson_sampling_contextual_alloc <- TSLINUCB(dt,visitor_reward, iter = 1000)
cum_reg_thompson_sampling_contextual_alloc <- cumulativeRegretAverage(thompson_sampling_contextual_alloc$choice,visitor_reward,dt = dt,message_tree = TRUE)
library(bandit4abtest)
?ctree
####Non contextual policy
if(is.na(dt) == TRUE){
max_average_regret = max(colMeans(visitor_reward))
regret <- c()
for (i in 1:nrow(visitor_reward)) {
regret[i] <- max_average_regret - mean(visitor_reward[1:i, as.integer(choice[i])])
if(regret[i] < 0)  regret[i] = 0
}
}
###Contextual policy
if(is.na(dt) == FALSE){
####Model construction
library(partykit)
#Change the type of data
temp <-changeDataTypeForCtreeUCB(dt=dt,visitor_reward=visitor_reward,is_reward_are_boolean=IsRewardAreBoolean)
dt <- temp$dt
#if reward is boolean, data will be modify temporary
temp.visitor_reward <- temp$visitor_reward
ctree_models <- c()
for(i in 1:ncol(visitor_reward)){
### learning  ###
#Generate formula and tree
ctree_models[[i]] <-  ctree_formula_generate(dt = dt,
visitor_reward = temp.visitor_reward,
ctree_control_val = ctree_control(teststat = c("quadratic")),
arm_for_learn = colnames(visitor_reward[i]),
explanatory_variable= explanatory_variable,
learn_size = nrow(dt))
if( message_tree == TRUE) plot(ctree_models[i]$)
}
regret <- averageRegret(choice= as.vector(choice), visitor_reward,dt,ctree_models,isRewardAreBoolean=IsRewardAreBoolean)
}
i
i=1
### learning  ###
#Generate formula and tree
ctree_models[[i]] <-  ctree_formula_generate(dt = dt,
visitor_reward = temp.visitor_reward,
ctree_control_val = ctree_control(teststat = c("quadratic")),
arm_for_learn = colnames(visitor_reward[i]),
explanatory_variable= explanatory_variable,
learn_size = nrow(dt))
####Model construction
library(partykit)
#Change the type of data
temp <-changeDataTypeForCtreeUCB(dt=dt,visitor_reward=visitor_reward,is_reward_are_boolean=IsRewardAreBoolean)
dt <- temp$dt
#if reward is boolean, data will be modify temporary
temp.visitor_reward <- temp$visitor_reward
ctree_models <- c()
IsRewardAreBoolean=FALSE
#Change the type of data
temp <-changeDataTypeForCtreeUCB(dt=dt,visitor_reward=visitor_reward,is_reward_are_boolean=IsRewardAreBoolean)
dt <- temp$dt
#if reward is boolean, data will be modify temporary
temp.visitor_reward <- temp$visitor_reward
ctree_models <- c()
i=1
### learning  ###
#Generate formula and tree
ctree_models[[i]] <-  ctree_formula_generate(dt = dt,
visitor_reward = temp.visitor_reward,
ctree_control_val = ctree_control(teststat = c("quadratic")),
arm_for_learn = colnames(visitor_reward[i]),
explanatory_variable= explanatory_variable,
learn_size = nrow(dt))
explanatory_variable=colnames(dt)
### learning  ###
#Generate formula and tree
ctree_models[[i]] <-  ctree_formula_generate(dt = dt,
visitor_reward = temp.visitor_reward,
ctree_control_val = ctree_control(teststat = c("quadratic")),
arm_for_learn = colnames(visitor_reward[i]),
explanatory_variable= explanatory_variable,
learn_size = nrow(dt))
if( message_tree == TRUE) plot(ctree_models[i]$)
ctree_models[1]
plot(ctree_models[1])
print(ctree_models[1])
library(partykit)
print(ctree_models[1])
print(ctree_models[[1]])
print(ctree_models[[1]])
library(bandit4abtest)
size.tot = 2000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
x3 = runif(size.tot, min=0, max=10)
x4 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2,x3,x4)
#arm reward
arm_1 <-  as.vector(c(-1,9,-8,4))
K1 = crossprod(t(dt),arm_1)
arm_2 <-  as.vector(c(-1,2,1,0))
K2 = crossprod(t(dt),arm_2)
arm_3 <-  as.vector(c(-1,-5,1,10))
K3 = crossprod(t(dt),arm_3)
visitor_reward <-  data.frame(K1,K2,K3)
dt <- as.data.frame(dt)
thompson_sampling_contextual_alloc <- TSLINUCB(dt,visitor_reward, iter = 1000)
cum_reg_thompson_sampling_contextual_alloc <- cumulativeRegretAverage(thompson_sampling_contextual_alloc$choice,visitor_reward,dt = dt,message_tree = TRUE)
size.tot = 2000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2,x3,x4)
#arm reward
arm_1 <-  as.vector(c(-1,1))
size.tot = 2000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2)
#arm reward
arm_1 <-  as.vector(c(-1,1))
K1 = crossprod(t(dt),arm_1)
arm_2 <-  as.vector(c(1,-1))
K2 = crossprod(t(dt),arm_2)
arm_3 <-  as.vector(c(-0.5,-0.5))
K3 = crossprod(t(dt),arm_3)
visitor_reward <-  data.frame(K1,K2,K3)
dt <- as.data.frame(dt)
summary(visitor_reward)
dt <- as.data.frame(dt)
thompson_sampling_contextual_alloc <- TSLINUCB(dt,visitor_reward, iter = 1000)
cum_reg_thompson_sampling_contextual_alloc <- cumulativeRegretAverage(thompson_sampling_contextual_alloc$choice,visitor_reward,dt = dt,message_tree = TRUE)
install.packages(c("ape", "backports", "BH", "broom", "callr", "car", "carData", "cli", "covr", "crosstalk", "datasets.load", "dbplyr", "devtools", "DiagrammeR", "dplyr", "DT", "ellipse", "ellipsis", "fansi", "farver", "forcats", "foreach", "fs", "future", "ggplot2", "ggpubr", "git2r", "glue", "gtools", "haven", "hms", "httpuv", "isoband", "janitor", "jsonlite", "keras", "knitr", "later", "lava", "lifecycle", "lme4", "lubridate", "maptools", "mime", "modelr", "mvtnorm", "openxlsx", "padr", "partykit", "PerformanceAnalytics", "phytools", "pillar", "pkgbuild", "pkgload", "plotly", "plotrix", "plyr", "prettyunits", "pROC", "processx", "promises", "proxy", "ps", "purrr", "quantmod", "raster", "Rcpp", "RcppArmadillo", "RcppParallel", "recipes", "reshape2", "reticulate", "rex", "rlang", "rmarkdown", "roxygen2", "rsample", "rstudioapi", "rversions", "scales", "shinyWidgets", "sp", "SQUAREM", "stringi", "tensorflow", "tibble", "tibbletime", "tidyquant", "tidyr", "tidyselect", "timetk", "tinytex", "TraMineR", "usethis", "V8", "vctrs", "withr", "xfun", "xml2", "xts", "yaml", "yardstick", "zoo"))
