node_id_dataframe
node_id_dataframe$Id[i]= i+1
node_id_dataframe
node_id_dataframe$Type[i] = "Edge"
node_id_dataframe
node_id_dataframe$Name[i]= paste( "Edge", as.character(cn$edges$antecedent[j]), as.character(cn$edges$consequent[j]), sep="_")
temp = node_id_dataframe$Name == paste( as.character(cn$edges$antecedent[j]), as.character(cn$edges$consequent[j]), sep="_")
node_id_dataframe$from[i] = node_id_dataframe[temp,]$Id
temp = node_id_dataframe$Name ==  as.character(cn$edges$consequent[j])
node_id_dataframe$to[i] = node_id_dataframe[temp,]$Id
j=j+1
}
library(tidyverse)
###############
###############
sink("monfichier.grml")
cat("<model id=\"1\" formalismUrl=\"http://formalisms.cosyverif.org/sptgd-net.fml\">") # le r?sultat de la commande est ecrit dans monfichier.txt
cat("<attribute name=\"declaration\">")
cat("<attribute name=\"constants\">")
cat("<attribute name=\"intConsts\"> </attribute>")
cat("<attribute name=\"realConsts\">")
##### loop : declare constant ###
cat("<attribute name=\"realConst\">")
cat("<attribute name=\"name\">alpha1</attribute>")
cat("<attribute name=\"expr\">")
cat("<attribute name=\"numValue\">0.4</attribute>")
cat("</attribute>")
cat("</attribute>")
##### loop : declare constant ###
for( i in node_id_dataframe[node_id_dataframe$Type == "Transition", ]$Id ){
#i =  levels(as.factor(as.character(node_id_dataframe$Value)))[2]
cat("<attribute name=\"realConst\">")
#cat(paste("<attribute name=\"name\">alpha",str_replace(i,"\\.","p"),"</attribute>",sep=''))
cat(paste("<attribute name=\"name\">alpha",node_id_dataframe[ node_id_dataframe$Id == i, ]$Name,"</attribute>",sep=''))
cat("<attribute name=\"expr\">")
cat(paste("<attribute name=\"numValue\">",node_id_dataframe[ node_id_dataframe$Id == i, ]$Value,"</attribute>",sep=''))
cat("</attribute>")
cat("</attribute>")
}
##### end loop #####
##### end loop #####
cat("</attribute>")
cat(" </attribute>")
cat(" </attribute>")
###### end attribute  ######
temp = 1
for(i in 1:nrow(node_id_dataframe)){
if( node_id_dataframe$Type[i] == "Noeud" ){
nb_token = sum( cn$edges[cn$edges$antecedent == node_id_dataframe$Name[i],]$n  )
print_node_xml(id=node_id_dataframe$Id[i],x=temp,y=temp,name_node=node_id_dataframe$Name[i],int_value = nb_token)
temp = temp + 1
}
}
for(i in 1:nrow(node_id_dataframe)){
if( node_id_dataframe$Type[i] == "Transition" ){
print_transition_xml(id=node_id_dataframe$Id[i],x=temp,y=temp,name_transition=node_id_dataframe$Name[i],
value = paste("alpha",node_id_dataframe$Name[i],sep='')
#  value = "alpha1"
)
temp = temp + 1
}
}
for(i in 1:nrow(node_id_dataframe)){
if( node_id_dataframe$Type[i] == "Edge" ){
print_arc_xml(id=node_id_dataframe$Id[i],name_elt_1 = node_id_dataframe$from[i], name_elt_2 = node_id_dataframe$to[i])
temp = temp + 1
}
}
cat("</model>")
sink()
### without temporary steps
################
# FUCNTION #
## node ##
print_node_xml <- function(id,x,y,name_node,int_value){
cat(paste("<node id=\"",id,"\" nodeType=\"place\" x=\"",x,"\" y=\"",y,"\">",sep = ""))
cat(paste("<attribute name=\"name\">",name_node,"</attribute>",sep=""))
cat("<attribute name=\"marking\">")
cat("<attribute name=\"expr\">")
cat(paste("<attribute name=\"intValue\">",int_value,"</attribute>",sep=""))
cat("</attribute>")
cat("</attribute>")
cat("</node>")
}
## transition ##
print_transition_xml <- function(id,x,y,name_transition, value){
cat(paste("<node id=\"",id,"\" nodeType=\"transition\" x=\"",x,"\" y=\"",y,"\">",sep = ""))
cat(paste("<attribute name=\"name\">",name_transition,"</attribute>",sep=""))
cat("<attribute name=\"distribution\">")
cat("<attribute name=\"type\">EXPONENTIAL</attribute>")
cat("<attribute name=\"param\">")
cat("<attribute name=\"expr\">")
cat(paste("<attribute name=\"name\">",value,"</attribute>",sep=""))
cat("</attribute>")
cat("</attribute>")
cat("</attribute>")
cat("</node>")
}
## arc ##
print_arc_xml <- function(id,name_elt_1, name_elt_2){
cat(paste("<arc id=\"",id,"\" arcType=\"arc\" source=\"",name_elt_1,"\" target=\"",name_elt_2,"\">",sep = ""))
cat(paste("<attribute name=\"valuation\">",sep=""))
cat("<attribute name=\"expr\">")
cat("<attribute name=\"intValue\">1</attribute>")
cat("</attribute>")
cat("</attribute>")
cat("</arc>")
}
###############################################################################
#objet contenant les noeud , et transition)
node_id_dataframe = data.frame(Id=numeric(nrow(as.data.frame(cn$nodes))),
Type=as.character(nrow(as.data.frame(cn$nodes))),
Name=character(nrow(as.data.frame(cn$nodes))),
Value = numeric(nrow(as.data.frame(cn$nodes))))
node_id_dataframe$Type = as.character(node_id_dataframe$Type)
node_id_dataframe$Name = as.character(node_id_dataframe$Name)
#Remplissage de node_id_dataframe
### Ajout noeud ###
for(i in 1:nrow(as.data.frame(cn$nodes)) ){
node_id_dataframe[i,] =  rep(NA,4)
# node_id_dataframe
node_id_dataframe$Id[i]= i+1
#  node_id_dataframe
node_id_dataframe$Type[i] = "Noeud"
#  node_id_dataframe
node_id_dataframe$Name[i]= as.character(cn$nodes$act[i])
#  node_id_dataframe
}
j=1
for(i in (nrow(node_id_dataframe)+1):(nrow(node_id_dataframe)+nrow(as.data.frame(cn$edges)))){
node_id_dataframe[i,] =  rep(NA,4)
#  node_id_dataframe
node_id_dataframe$Id[i]= i+1
#  node_id_dataframe
node_id_dataframe$Type[i] = "Transition"
#  node_id_dataframe
node_id_dataframe$Name[i]= paste( as.character(cn$edges$antecedent[j]), as.character(cn$edges$consequent[j]), sep="_")
node_id_dataframe$Value[i] = cn$edges$label_numeric[j]
j=j+1
}
#node_id_dataframe
### Ajout Arc ###
node_id_dataframe$from = 0
node_id_dataframe$to = 0
j=1
for(i in (nrow(node_id_dataframe)+1):(nrow(node_id_dataframe)+nrow(as.data.frame(cn$edges)))){
node_id_dataframe[i,] =  rep(NA,6)
node_id_dataframe
node_id_dataframe$Id[i]= i+1
node_id_dataframe
node_id_dataframe$Type[i] = "Edge"
node_id_dataframe
node_id_dataframe$Name[i]= paste( "Edge", as.character(cn$edges$antecedent[j]), as.character(cn$edges$consequent[j]), sep="_")
temp = node_id_dataframe$Name == paste( as.character(cn$edges$antecedent[j]), as.character(cn$edges$consequent[j]), sep="_")
node_id_dataframe$from[i] = node_id_dataframe[temp,]$Id
temp = node_id_dataframe$Name ==  as.character(cn$edges$consequent[j])
node_id_dataframe$to[i] = node_id_dataframe[temp,]$Id
j=j+1
}
library(tidyverse)
###############
###############
sink("monfichier.grml")
cat("<model id=\"1\" formalismUrl=\"http://formalisms.cosyverif.org/sptgd-net.fml\">") # le r?sultat de la commande est ecrit dans monfichier.txt
cat("<attribute name=\"declaration\">")
cat("<attribute name=\"constants\">")
cat("<attribute name=\"intConsts\"> </attribute>")
cat("<attribute name=\"realConsts\">")
##### loop : declare constant ###
cat("<attribute name=\"realConst\">")
cat("<attribute name=\"name\">alpha1</attribute>")
cat("<attribute name=\"expr\">")
cat("<attribute name=\"numValue\">0.4</attribute>")
cat("</attribute>")
cat("</attribute>")
##### loop : declare constant ###
for( i in node_id_dataframe[node_id_dataframe$Type == "Transition", ]$Id ){
#i =  levels(as.factor(as.character(node_id_dataframe$Value)))[2]
cat("<attribute name=\"realConst\">")
#cat(paste("<attribute name=\"name\">alpha",str_replace(i,"\\.","p"),"</attribute>",sep=''))
cat(paste("<attribute name=\"name\">alpha",node_id_dataframe[ node_id_dataframe$Id == i, ]$Name,"</attribute>",sep=''))
cat("<attribute name=\"expr\">")
cat(paste("<attribute name=\"numValue\">",node_id_dataframe[ node_id_dataframe$Id == i, ]$Value,"</attribute>",sep=''))
cat("</attribute>")
cat("</attribute>")
}
##### end loop #####
##### end loop #####
cat("</attribute>")
cat(" </attribute>")
cat(" </attribute>")
###### end attribute  ######
temp = 1
for(i in 1:nrow(node_id_dataframe)){
if( node_id_dataframe$Type[i] == "Noeud" ){
nb_token = sum( cn$edges[cn$edges$antecedent == node_id_dataframe$Name[i],]$n  )
print_node_xml(id=node_id_dataframe$Id[i],x=temp,y=temp,name_node=node_id_dataframe$Name[i],int_value = nb_token)
temp = temp + 1
}
}
for(i in 1:nrow(node_id_dataframe)){
if( node_id_dataframe$Type[i] == "Transition" ){
print_transition_xml(id=node_id_dataframe$Id[i],x=temp,y=temp,name_transition=node_id_dataframe$Name[i],
value = paste("alpha",node_id_dataframe$Name[i],sep='')
#  value = "alpha1"
)
temp = temp + 1
}
}
for(i in 1:nrow(node_id_dataframe)){
if( node_id_dataframe$Type[i] == "Edge" ){
print_arc_xml(id=node_id_dataframe$Id[i],name_elt_1 = node_id_dataframe$from[i], name_elt_2 = node_id_dataframe$to[i])
temp = temp + 1
}
}
cat("</model>")
sink()
library(data.table)
library(janitor)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(viridis)
library(bupaR)
library(TraMineR)
library(pbapply)
library(pm4py)
library(heuristicsmineR)
setwd("~/github/yourdata/Actitrac-LJ")
source("multiplot.R") #Plusieurs ggplot sur une seule page
source("plotly_cols.R") #Couleurs de base de plotly
source("popvar.R") #variance de la population (=(n-1)/n*var(X))
source("popsd.R") #Ecart-type de la population (=(sqrt(n-1)/n)*sd(X))
source("sn.R") #Estimateur Sn de Rousseeuw
source("qn.R") #Estimateur Qn de Rousseeuw
source("suffixes.R")
source("distMRA.R")
source("LCP.R") #Fonction Longest Common Prefixes
source("findmaxr1.R") #Fonction qui trouve les repetitions maximales pour une trace
source("findmaxr.R") #Fonction qui trouve les repetitions maximales de chaque trace dans un log
source("actitrac.R") #Le bijou
source("pm_clusterwise.R") #Calcule un modele de process mining par cluster a partir d'un clustering via actitrac
source("evaluation_clus.R") #Evaluation du clustering (entropie, moyenne ponderee des scores f1, PTCD moyenne)
source("cluster_id.R")
source("distMRA.R")
edf<-fread("EdF Anonymised/EdF Anonymised.csv")%>%
clean_names%>%
convert_timestamps("time_stamp1")%>%
simple_eventlog("case",
"status_code",
"time_stamp1")
#lien du jeu de donn?es : https://data.4tu.nl/repository/uuid:895b26fb-6f25-46eb-9e48-0dca26fcd030
bpi<-xesreadR::read_xes("PrepaidTravelCost.xes")
n_cases(bpi)
n_traces(bpi)
set.seed(526)
################################################################ Fonction ActiTraC ################################################################
#Si edf, plus simple de prendre un ?chantillon :
test<-sample_n(edf,1000)
#puis :
actest<-actitrac(test,nb_clus=2,mcs=10,tf=.9,threshold_dependency=.95,endpoints_connected=TRUE)
#Pour l'instant, fait avec bpi
#actest<-actitrac(bpi,mcs=5,nb_clus=3,tf=.95,threshold_dependency=.95,endpoints_connected=TRUE,N=TRUE)
actest$clustered_log$clusters[!duplicated(actest$clustered_log$CASE_concept_name)]%>%table
# Cr?ation des mod?les
fit<-pm_clusterwise(actest,discovery="heuristics",endpoints_connected=TRUE,threshold=.8,type=causal_performance(units="hours"))
fit$model1
fit$model2
fit$model3
cn <- fit$model2
#Evaluation des clusters. Demande de les convertir en petri nets
#fit$model1 <-as.petrinet(fit$model1)
#fit$model2 <-as.petrinet(fit$model2)
#fit$model3 <-as.petrinet(fit$model3)
#aze<-actest$clustered_log
#evaluation_clus(x=aze,clus_index=cluster_id(aze),model=fit,discovery=fit$discovery) #fitness weighted average (par defaut)
#evaluation_clus(x=aze,clus_index=cluster_id(aze),method="ptcda",model=fit,discovery=fit$discovery) #Place/transition connection degree average
#evaluation_clus(x=aze,clus_index=cluster_id(aze),method="hcs",class_index=10) #Cluster set entropy
library(bandit4abtest)
library(bandit4abtest)
set.seed(4434)
V1 <- rbinom(1000, 1, 0.6)  # Generates 5000 numbers from a uniform distribution with mean 0.75
V2 <- rbinom(1000, 1, 0.7)
V3 <- rbinom(1000, 1, 0.5)
V4 <- rbinom(1000, 1, 0.3)
V5 <- rbinom(1000, 1, 0.9)
V1
visitorReward <- as.data.frame( cbind(V1,V2,V3,V4,V5) )
View(visitorReward)
ucb_alloc  <- UCB(visitorReward,alpha = 1)
View(ucb_alloc)
ucb_alloc$S
ucb_alloc$choice
ucb_alloc$proba
ucb_alloc$time
ucb_alloc$theta_hat
ucb_alloc$theta
cum_reg_ucb_alloc  <- cumulativeRegretAverage(ucb_alloc$choice,visitorReward)
epsilonGreedy_alloc <- EpsilonGreedy(visitorReward,epsilon  = 0.05)
epsilonGreedy_alloc$S
ucb_alloc$S
epsilonGreedy_alloc$choice
epsilonGreedy_alloc$time
epsilonGreedy_alloc$theta_hat
cum_reg_epsilonGreedy_alloc  <- cumulativeRegretAverage(epsilonGreedy_alloc$choice,visitorReward)
thompson_sampling_alloc <- ThompsonSampling(visitorReward)
thompson_sampling_alloc$S
epsilonGreedy_alloc$S
ucb_alloc$S
thompson_sampling_alloc <- ThompsonSampling(visitorReward)
cum_reg_thompson_sampling_alloc <- cumulativeRegretAverage(thompson_sampling_alloc$choice,visitorReward)
thompson_sampling_alloc$time
random_alloc <- UniformBandit(visitorReward)
cum_reg_random_alloc <- cumulativeRegretAverage(random_alloc$choice,visitorReward)
random_alloc$S
random_alloc$choice
library(ggplot2)
comp_reg <- data.frame(cbind(cum_reg_ucb_alloc,
cum_reg_epsilonGreedy_alloc,
cum_reg_thompson_sampling_alloc,
cum_reg_random_alloc ))
cum_reg_ucb_alloc
comp_reg <- data.frame(cbind(cum_reg_ucb_alloc,
cum_reg_epsilonGreedy_alloc,
cum_reg_thompson_sampling_alloc,
cum_reg_random_alloc ))
ggplot(comp_reg, aes(c(1:nrow(comp_reg)), y = value, color = Algorithm)) +
geom_line(linetype="dashed",aes(y = cum_reg_ucb_alloc, col = "UCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_epsilonGreedy_alloc, col = "Epsilon Greedy"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_thompson_sampling_alloc, col = "Thompson Sampling"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_random_alloc, col = "Random"),size = 0.5) +
scale_colour_manual(values =  c("UCB"="brown","Epsilon Greedy"="orange","Thompson Sampling"="green","Random"="black"))+
xlab("Time T") +
ylab("Cumulative regret")
#################
rm(list = ls())
#Contextual
size.tot = 10000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2)
View(dt)
#arm reward
arm_1 <-  as.vector(c(0.1,-0.3))
K1 = 1/(1+exp(- crossprod(t(dt),arm_1))) # inverse logit transform of linear predictor
View(K1)
View(dt)
K1 = vapply(K1, function(x) rbinom(1, 1, x), as.integer(1L))
summary(K1)
arm_2 <-  as.vector(c(0.3,-0.4))
arm_2 <-  as.vector(c(0.3,-0.4))
K2 = 1/(1+exp(- crossprod(t(dt),arm_2))) # inverse logit transform of linear predictor
summary(K2)
K2 = vapply(K2, function(x) rbinom(1, 1, x), as.integer(1L))
summary(K2)
visitor_reward <-  data.frame(K1,K2)
dt <- as.data.frame(dt)
summary(visitor_reward)
thompson_sampling_contextual_alloc <- TSLINUCB(dt=dt,visitor_reward)
cum_reg_thompson_sampling_contextual_alloc <- cumulativeRegretAverage(thompson_sampling_contextual_alloc$choice,visitor_reward,dt = dt)
linucb_contextual_alloc <- LINUCB(dt,visitor_reward )
cum_reg_linucb_contextual_alloc <- cumulativeRegretAverage(linucb_contextual_alloc$choice,visitor_reward,dt = dt)
kernelucb_contextual_alloc <- kernelucb(dt, visitor_reward, update_val = 500)
cum_reg_kernelucb_contextual_alloc <- cumulativeRegretAverage(kernelucb_contextual_alloc$choice,visitor_reward,dt=dt)
cum_reg_kernelucb_contextual_alloc <- cumulativeRegret(kernelucb_contextual_alloc$choice,visitor_reward,dt=dt)
cum_reg_thompson_sampling_contextual_alloc <- cumulativeRegret(thompson_sampling_contextual_alloc$choice,visitor_reward,dt = dt)
cum_reg_kernelucb_contextual_alloc <- cumulativeRegret(kernelucb_contextual_alloc$choice,visitor_reward,dt=dt)
random_alloc <- UniformBandit(visitor_reward)
cum_reg_random_alloc <- cumulativeRegret(random_alloc$choice,visitor_reward)
cum_reg_linucb_contextual_alloc <- cumulativeRegret(linucb_contextual_alloc$choice,visitor_reward,dt = dt)
cum_reg_linucb_contextual_alloc <- cumulativeRegret(linucb_contextual_alloc$choice,visitor_reward)
cum_reg_kernelucb_contextual_alloc <- cumulativeRegret(kernelucb_contextual_alloc$choice,visitor_reward)
cum_reg_random_alloc <- cumulativeRegret(random_alloc$choice,visitor_reward)
ucb_alloc  <- UCB( visitor_reward,alpha = 1)
cum_reg_ucb_alloc  <- cumulativeRegret(ucb_alloc$choice, visitor_reward)
cumulativeRegretAverage(ucb_alloc$choice, visitor_reward)
epsilonGreedy_alloc <- EpsilonGreedy( visitor_reward,epsilon  = 0.05)
cum_reg_epsilonGreedy_alloc  <- cumulativeRegret(epsilonGreedy_alloc$choice,visitor_reward)
thompson_sampling_alloc <- ThompsonSampling( visitor_reward)
cum_reg_thompson_sampling_alloc <- cumulativeRegret(thompson_sampling_alloc$choice,visitor_reward)
library(ggplot2)
comp_reg <- data.frame(cbind(cum_reg_linucb_contextual_alloc,
cum_reg_kernelucb_contextual_alloc,
cum_reg_thompson_sampling_contextual_alloc,
cum_reg_random_alloc,
cum_reg_ucb_alloc,
cum_reg_epsilonGreedy_alloc,
cum_reg_thompson_sampling_alloc))
ggplot(comp_reg, aes(c(1:nrow(comp_reg)), y = value, color = Algorithm)) +
geom_line(linetype="dashed",aes(y = cum_reg_ucb_alloc, col = "UCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_epsilonGreedy_alloc, col = "Epsilon Greedy"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_thompson_sampling_alloc, col = "Thompson Sampling"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_random_alloc, col = "Random"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_thompson_sampling_contextual_alloc, col = "Contextual TS"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_linucb_contextual_alloc, col = "LINUCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_kernelucb_contextual_alloc, col = "KernelUCB"),size = 0.5) +
scale_colour_manual(values =  c("UCB"="brown","Epsilon Greedy"="orange","Thompson Sampling"="green","Random"="black", "Contextual TS"="dark green", "LINUCB"="blue","KernelUCB"= "purple"))+
xlab("Time T") +
ylab("Cumulative regret")
##### Pairewise #####
set.seed(1234)
size.tot <- 10000
x <- seq(0, 5, 0.01)
x1<- sample(x, size.tot, replace = TRUE, prob = NULL)
arm_1 <-  as.vector(c(2,-1,1.5,0))
K1 <- (x1 < 1 ) * arm_1[4]  +
(x1 >= 1 & x1 < 2 ) * arm_1[1]  +
(x1 >= 2 & x1 < 3) * arm_1[2]  +
(x1 >= 3 & x1 < 4) * arm_1[3]  +
(x1 >= 4) * arm_1[4]
plot(x1, K1)
arm_2 <-  as.vector(c(1.5,-0.5,1.25,0))
K2 <- (x1 < 1 ) * arm_2[4]  +
(x1 >= 1 & x1 < 2 ) * arm_2[1]  +
(x1 >= 2 & x1 < 3) * arm_2[2]  +
(x1 >= 3 & x1 < 4) * arm_2[3]  +
(x1 >= 4) * arm_2[4]
plot(x1, K2)
#covariate without interest
x2<- sample(x, size.tot, replace = TRUE, prob = NULL)
#Results for each variation
visitor_reward <-  data.frame(K1,K2 )
summary(visitor_reward)
dt <- as.data.frame(cbind(x1,x2))
controle_param = ctreeucb_parameters_control_default(dt=dt, visitor_reward=visitor_reward,learn_size=1500,  alpha=1, ctree_control_val= partykit::ctree_control(teststat = "quadratic"))
#covariate without interest
x2<- sample(x, size.tot, replace = TRUE, prob = NULL)
#Results for each variation
visitor_reward <-  data.frame(K1,K2 )
summary(visitor_reward)
dt <- as.data.frame(cbind(x1,x2))
controle_param = ctreeucb_parameters_control_default(dt=dt, visitor_reward=visitor_reward,learn_size=1500,  alpha=1, ctree_control_val= partykit::ctree_control(teststat = "quadratic"))
ctreeucb_alloc <- ctreeucb(dt, visitor_reward, ctree_parameters_control = controle_param )
#take data for online ab test for other algorithm
first <- ctreeucb_alloc$first_train_element
last <- nrow(visitor_reward)
dt.abtest <- dt[first:last,]
visitor_reward.abtest <- visitor_reward[first:last,]
cum_reg_ctreeucb_alloc <- cumulativeRegret(ctreeucb_alloc$choice,visitor_reward.abtest)
linucb_contextual_alloc <- LINUCB(dt.abtest,visitor_reward.abtest )
cum_reg_linucb_contextual_alloc <- cumulativeRegretAverage(linucb_contextual_alloc$choice,visitor_reward.abtest,dt = dt.abtest)
kernelucb_contextual_alloc <- kernelucb(dt.abtest, visitor_reward.abtest, update_val = 500)
cum_reg_kernelucb_contextual_alloc <- cumulativeRegretAverage(kernelucb_contextual_alloc$choice,visitor_reward.abtest,dt=dt.abtest)
max(cum_reg_kernelucb_contextual_alloc)
random_alloc <- UniformBandit(visitor_reward.abtest)
cum_reg_random_alloc <- cumulativeRegret(random_alloc$choice,visitor_reward.abtest)
ucb_alloc  <- UCB( visitor_reward.abtest,alpha = 1)
cum_reg_ucb_alloc  <- cumulativeRegret(ucb_alloc$choice, visitor_reward.abtest)
epsilonGreedy_alloc <- EpsilonGreedy( visitor_reward.abtest,epsilon  = 0.05)
cum_reg_epsilonGreedy_alloc  <- cumulativeRegret(epsilonGreedy_alloc$choice,visitor_reward.abtest)
library(data.table)
library(janitor)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(viridis)
library(bupaR)
library(TraMineR)
library(pbapply)
library(pm4py)
library(heuristicsmineR)
setwd("~/github/yourdata/Actitrac-LJ")
source("multiplot.R") #Plusieurs ggplot sur une seule page
source("plotly_cols.R") #Couleurs de base de plotly
source("popvar.R") #variance de la population (=(n-1)/n*var(X))
source("popsd.R") #Ecart-type de la population (=(sqrt(n-1)/n)*sd(X))
source("sn.R") #Estimateur Sn de Rousseeuw
source("qn.R") #Estimateur Qn de Rousseeuw
source("suffixes.R")
source("distMRA.R")
source("LCP.R") #Fonction Longest Common Prefixes
source("findmaxr1.R") #Fonction qui trouve les repetitions maximales pour une trace
source("findmaxr.R") #Fonction qui trouve les repetitions maximales de chaque trace dans un log
source("actitrac.R") #Le bijou
source("pm_clusterwise.R") #Calcule un modele de process mining par cluster a partir d'un clustering via actitrac
source("evaluation_clus.R") #Evaluation du clustering (entropie, moyenne ponderee des scores f1, PTCD moyenne)
source("cluster_id.R")
source("distMRA.R")
edf<-fread("EdF Anonymised/EdF Anonymised.csv")%>%
clean_names%>%
convert_timestamps("time_stamp1")%>%
simple_eventlog("case",
"status_code",
"time_stamp1")
#lien du jeu de donn?es : https://data.4tu.nl/repository/uuid:895b26fb-6f25-46eb-9e48-0dca26fcd030
bpi<-xesreadR::read_xes("PrepaidTravelCost.xes")
n_cases(bpi)
n_traces(bpi)
set.seed(526)
################################################################ Fonction ActiTraC ################################################################
#Si edf, plus simple de prendre un ?chantillon :
test<-sample_n(edf,1000)
#puis :
actest<-actitrac(test,nb_clus=2,mcs=10,tf=.9,threshold_dependency=.95,endpoints_connected=TRUE)
#Pour l'instant, fait avec bpi
#actest<-actitrac(bpi,mcs=5,nb_clus=3,tf=.95,threshold_dependency=.95,endpoints_connected=TRUE,N=TRUE)
actest$clustered_log$clusters[!duplicated(actest$clustered_log$CASE_concept_name)]%>%table
# Cr?ation des mod?les
fit<-pm_clusterwise(actest,discovery="heuristics",endpoints_connected=TRUE,threshold=.8,type=causal_performance(units="hours"))
fit$model1
fit$model2
fit$model3
cn <- fit$model2
#Evaluation des clusters. Demande de les convertir en petri nets
#fit$model1 <-as.petrinet(fit$model1)
#fit$model2 <-as.petrinet(fit$model2)
#fit$model3 <-as.petrinet(fit$model3)
#aze<-actest$clustered_log
#evaluation_clus(x=aze,clus_index=cluster_id(aze),model=fit,discovery=fit$discovery) #fitness weighted average (par defaut)
#evaluation_clus(x=aze,clus_index=cluster_id(aze),method="ptcda",model=fit,discovery=fit$discovery) #Place/transition connection degree average
#evaluation_clus(x=aze,clus_index=cluster_id(aze),method="hcs",class_index=10) #Cluster set entropy
