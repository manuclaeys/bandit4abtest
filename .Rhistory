dt <-  as.data.frame(cbind(x1,y$time_series,y$cluster))
colnames(dt) <- c("x1","time_series","cluster")
visitor_reward <-  data.frame(rep(0,size.tot),rep(0,size.tot),rep(0,size.tot))
colnames(visitor_reward) = c("K1","K2","K3")
for(i in 1:nrow(dt)) {
if(dt$cluster[i] == 1){
visitor_reward$K1[i] = 3 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1 + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 2){
visitor_reward$K1[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 3 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1 + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 3){
visitor_reward$K1[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 3 + rnorm(1,mean = 0, sd =1)
}
}
dt$x1 <- as.numeric(dt$x1)
temp <- dt$cluster
dt$cluster <- NULL
K=ncol(visitor_reward)
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward)
listSerie = c("time_series")
listKCentroids=c(3)
listInteger=c("x1")
precent = 0.7
res <- DBALINUCB_rejection_sampling(dt, visitor_reward,
alpha=1, K=ncol(visitor_reward),
listSerie, listKCentroids ,
learn_size = as.integer(nrow(dt)*precent),
IsRewardAreBoolean = FALSE ,
listCategorial=0 ,
listInteger=listInteger)
temp <- cumulativeRegret(res$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
nrow(visitor_reward[(as.integer(nrow(dt)*precent)):nrow(dt),])
length(res$choice)
res$theta_hat
res$theta
dt$meanTS <- 0
#Moyenne
for (i in 1:nrow(dt)) {
dt$meanTS[i] = mean(dt$time_series[[i]])
}
newdt <- dt
newdt$time_series <- NULL
resLinUCB <- LinucbBanditObjectEvaluation(newdt[as.integer(nrow(dt)*precent):nrow(dt),],visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
temp2 <- cumulativeRegret(resLinUCB$linucb_bandit_alloc$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
max(temp)
max(temp2)
alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = length(t))
i
i=1
alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = length(t))
plot (alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = length(t)))
plot (alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = 10000))
i=2
plot (alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = 10000))
i=3
plot (alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = 10000))
library(bandit4abtest)
?UCB
## Generates 10000 numbers from 2 binomial  distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
K1
K2 <- rbinom(1000, 1, 0.7)
K2
mean(K1)
mean(K2)
## Define a dataframe of rewards
visitor_reward <- as.data.frame(cbind(K1,K2) )
View(visitor_reward)
#Run UCB algorithm
ucb_alloc  <- UCB(visitor_reward,alpha = 10)
ucb_alloc$S
barplot(table(ucb_alloc$choice),main = "Histogram of choices",xlab="arm")
#Upper bound for arm 2 according iterations (red line is the real mean)
plot(x=c(1:length(ucb_alloc$choice[ucb_alloc$choice==2])),
ucb_alloc$proba[ucb_alloc$choice==2],
type='l',xlab = 'Time',ylab = 'Upper bound of arm 2')
lines(c(1:length(ucb_alloc$choice[ucb_alloc$choice==2])),rep(mean(K2),
length(ucb_alloc$choice[ucb_alloc$choice==2])),col="red")
ucb_alloc$time
ucb_alloc$theta_hat
ucb_alloc$theta
?UcbBanditObjectEvaluation
## Generates 1000 numbers from 2 binomial distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame(cbind(K1,K2) )
View(visitor_reward)
#Run UCB algorithm with policy evaluation
UcbBanditObjectEvaluation(visitor_reward,alpha = 1)
UcbBanditObjectEvaluation(visitor_reward,alpha = 1,average = TRUE,IsRewardAreBoolean = TRUE)
?KLUCB
## Generates 1000 numbers from 2 uniform distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame( cbind(K1,K2) )
klucb_alloc <- KLUCB(visitor_reward)
klucb_alloc$S
klucb_alloc$time
klucb_alloc$theta
klucb_alloc$theta_hat
?ThompsonSampling
## Generates 1000 numbers from 2 uniform distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame( cbind(K1,K2) )
ThompsonSampling(visitor_reward)
?LinucbBanditObjectEvaluation
size.tot = 1000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 4, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
x3 = runif(size.tot, min=0, max=10)
x4 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2,x3,x4)
View(dt)
#arm reward
arm_1 <-  as.vector(c(-1,9,-8,4))
K1 = 1/(1+exp(- crossprod(t(dt),arm_1))) # inverse logit transform of linear predictor
K1 = vapply(K1, function(x) rbinom(1, 1, x), as.integer(1L))
arm_1
k1
K1
arm_2 <-  as.vector(c(-1,2,1,0))
K2 = 1/(1+exp(- crossprod(t(dt),arm_2))) # inverse logit transform of linear predictor
K2 = vapply(K2, function(x) rbinom(1, 1, x), as.integer(1L))
arm_3 <-  as.vector(c(-1,-5,1,10))
K3 = 1/(1+exp(- crossprod(t(dt),arm_3)))
K3 = vapply(K3, function(x) rbinom(1, 1, x), as.integer(1L))
visitor_reward <-  data.frame(K1,K2,K3)
dt <- as.data.frame(dt)
LinucbBanditObjectEvaluation(dt,visitor_reward)
LinucbBanditObjectEvaluation(dt,visitor_reward,average = TRUE,IsRewardAreBoolean = TRUE)
LinucbBanditObjectEvaluation(dt,visitor_reward)
?ctreeucbBanditObjectEvaluation
##### Pairewise #####
set.seed(1234)
size.tot <- 10000
x <- seq(0, 5, 0.01)
x
x1<- sample(x, size.tot, replace = TRUE, prob = NULL)
x1
arm_1 <-  as.vector(c(2,-1,1.5,0))
K1 <- (x1 < 1 ) * arm_1[4]  +
(x1 >= 1 & x1 < 2 ) * arm_1[1]  +
(x1 >= 2 & x1 < 3) * arm_1[2]  +
(x1 >= 3 & x1 < 4) * arm_1[3]  +
(x1 >= 4) * arm_1[4]
plot(x1, K1)
arm_2 <-  as.vector(c(1.5,-0.5,1.25,0))
K2 <- (x1 < 1 ) * arm_2[4]  +
(x1 >= 1 & x1 < 2 ) * arm_2[1]  +
(x1 >= 2 & x1 < 3) * arm_2[2]  +
(x1 >= 3 & x1 < 4) * arm_2[3]  +
(x1 >= 4) * arm_2[4]
plot(x1, K2)
#covariate without interest
x2<- sample(x, size.tot, replace = TRUE, prob = NULL)
#Results for each variation
visitor_reward <-  data.frame(K1,K2 )
View(visitor_reward)
summary(visitor_reward)
dt <- as.data.frame(cbind(x1,x2))
controle_param = ctreeucb_parameters_control_default(dt=dt, visitor_reward=visitor_reward,learn_size=1500,  alpha=1, ctree_control_val= partykit::ctree_control(teststat = "quadratic"))
ctreeucb_bandit = ctreeucbBanditObjectEvaluation(dt=dt,visitor_reward,ctree_parameters_control = controle_param )
#take data for online ab test for other algorithm
first <-  ctreeucb_bandit$ctreeucb_bandit_alloc$first_train_element
last <- nrow(visitor_reward)
dt.abtest <- dt[first:last,]
visitor_reward.abtest <- visitor_reward[first:last,]
#compare with linucb bandit
linucb_bandit <- LinucbBanditObjectEvaluation(dt.abtest,visitor_reward.abtest)
size.tot = 9000
set.seed(4649)                          # this makes the example exactly reproducible
# Time series
alpha_list <- c(1,2,3)
beta_list <- c(0.5,0.1,-0.2)
theta_list <- c(0.8,0.2,0.5)
alpha_list <- c(1,1,1)
beta_list <- c(0.5,0.5,0.5)
theta_list <- c(10,500,1000)
y <- as.data.frame(c(1))
colnames(y) = "ID"
temp=1
for (j in 1:3000){
for (i in 1:length(alpha_list)){
n = sample(1:100,1)
t <- 1:n
ts <- alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = length(t))
y[temp, "time_series"][[1]] <- list(ts)
y[temp, "cluster"][[1]] <- i
y$ID[temp] = temp
temp = temp +1
}
}
#useless context
x1 = runif(size.tot, min=0, max=10)
dt <-  as.data.frame(cbind(x1,y$time_series,y$cluster))
colnames(dt) <- c("x1","time_series","cluster")
visitor_reward <-  data.frame(rep(0,size.tot),rep(0,size.tot),rep(0,size.tot))
View(dt)
colnames(visitor_reward) = c("K1","K2","K3")
for(i in 1:nrow(dt)) {
if(dt$cluster[i] == 1){
visitor_reward$K1[i] = 3 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1 + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 2){
visitor_reward$K1[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 3 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1 + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 3){
visitor_reward$K1[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 3 + rnorm(1,mean = 0, sd =1)
}
}
dt$x1 <- as.numeric(dt$x1)
temp <- dt$cluster
dt$cluster <- NULL
K=ncol(visitor_reward)
View(visitor_reward)
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward)
listSerie = c("time_series")
listKCentroids=c(3)
listInteger=c("x1")
precent = 0.7
res <- DBALINUCB_rejection_sampling(dt, visitor_reward,
alpha=1, K=ncol(visitor_reward),
listSerie, listKCentroids ,
learn_size = as.integer(nrow(dt)*precent),
IsRewardAreBoolean = FALSE ,
listCategorial=0 ,
listInteger=listInteger)
nrow(visitor_reward[(as.integer(nrow(dt)*precent)):nrow(dt),])
length(res$choice)
res$theta_hat
res$theta
dt$meanTS <- 0
#Moyenne
for (i in 1:nrow(dt)) {
dt$meanTS[i] = mean(dt$time_series[[i]])
}
newdt <- dt
newdt$time_series <- NULL
resLinUCB <- LinucbBanditObjectEvaluation(newdt[as.integer(nrow(dt)*precent):nrow(dt),],visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
View(resLinUCB)
temp2 <- cumulativeRegret(resLinUCB$linucb_bandit_alloc$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
max(temp)
max(temp2)
temp <- cumulativeRegret(res$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
max(temp)
max(temp2)
size.tot = 9000
set.seed(4649)                          # this makes the example exactly reproducible
# Time series
alpha_list <- c(1,2,3)
beta_list <- c(0.5,0.1,-0.2)
theta_list <- c(0.8,0.2,0.5)
alpha_list <- c(1,1,1)
beta_list <- c(0.5,0.5,0.5)
theta_list <- c(10,500,1000)
y <- as.data.frame(c(1))
colnames(y) = "ID"
temp=1
for (j in 1:3000){
for (i in 1:length(alpha_list)){
n = sample(1:100,1)
t <- 1:n
ts <- alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = length(t))
y[temp, "time_series"][[1]] <- list(ts)
y[temp, "cluster"][[1]] <- i
y$ID[temp] = temp
temp = temp +1
}
}
#useless context
x1 = runif(size.tot, min=0, max=10)
dt <-  as.data.frame(cbind(x1,y$time_series,y$cluster))
colnames(dt) <- c("x1","time_series","cluster")
visitor_reward <-  data.frame(rep(0,size.tot),rep(0,size.tot),rep(0,size.tot))
colnames(visitor_reward) = c("K1","K2","K3")
for(i in 1:nrow(dt)) {
if(dt$cluster[i] == 1){
visitor_reward$K1[i] = 3 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1 + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 2){
visitor_reward$K1[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 3 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1 + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 3){
visitor_reward$K1[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1 + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 3 + rnorm(1,mean = 0, sd =1)
}
}
dt$x1 <- as.numeric(dt$x1)
temp <- dt$cluster
dt$cluster <- NULL
K=ncol(visitor_reward)
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward)
listSerie = c("time_series")
listKCentroids=c(6)
library(bandit4abtest)
K=ncol(visitor_reward)
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward)
listSerie = c("time_series")
listKCentroids=c(6)
listInteger=c("x1")
precent = 0.7
res <- DBALINUCB_rejection_sampling(dt, visitor_reward,
alpha=1, K=ncol(visitor_reward),
listSerie, listKCentroids ,
learn_size = as.integer(nrow(dt)*precent),
IsRewardAreBoolean = FALSE ,
listCategorial=0 ,
listInteger=listInteger)
temp <- cumulativeRegret(res$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
nrow(visitor_reward[(as.integer(nrow(dt)*precent)):nrow(dt),])
length(res$choice)
res$theta_hat
res$theta
listKCentroids=c(12)
listInteger=c("x1")
precent = 0.7
res <- DBALINUCB_rejection_sampling(dt, visitor_reward,
alpha=1, K=ncol(visitor_reward),
listSerie, listKCentroids ,
learn_size = as.integer(nrow(dt)*precent),
IsRewardAreBoolean = FALSE ,
listCategorial=0 ,
listInteger=listInteger)
temp <- cumulativeRegret(res$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
size.tot = 9000
set.seed(4649)                          # this makes the example exactly reproducible
# Time series
alpha_list <- c(1,2,3)
beta_list <- c(0.5,0.1,-0.2)
theta_list <- c(0.8,0.2,0.5)
alpha_list <- c(1,1,1)
beta_list <- c(0.5,0.5,0.5)
theta_list <- c(10,500,1000)
y <- as.data.frame(c(1))
colnames(y) = "ID"
temp=1
for (j in 1:3000){
for (i in 1:length(alpha_list)){
n = sample(1:100,1)
t <- 1:n
ts <- alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = length(t))
y[temp, "time_series"][[1]] <- list(ts)
y[temp, "cluster"][[1]] <- i
y$ID[temp] = temp
temp = temp +1
}
}
#useless context
x1 = runif(size.tot, min=0, max=10)
dt <-  as.data.frame(cbind(x1,y$time_series,y$cluster))
colnames(dt) <- c("x1","time_series","cluster")
visitor_reward <-  data.frame(rep(0,size.tot),rep(0,size.tot),rep(0,size.tot))
colnames(visitor_reward) = c("K1","K2","K3")
for(i in 1:nrow(dt)) {
if(dt$cluster[i] == 1){
visitor_reward$K1[i] = 3*mean(dt$time_series[[i]])+ rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 2){
visitor_reward$K1[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 3*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 3){
visitor_reward$K1[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 3*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
}
}
dt$x1 <- as.numeric(dt$x1)
temp <- dt$cluster
dt$cluster <- NULL
K=ncol(visitor_reward)
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward)
listSerie = c("time_series")
listKCentroids=c(12)
listInteger=c("x1")
precent = 0.7
listKCentroids=c(6)
listInteger=c("x1")
precent = 0.7
res <- DBALINUCB_rejection_sampling(dt, visitor_reward,
alpha=1, K=ncol(visitor_reward),
listSerie, listKCentroids ,
learn_size = as.integer(nrow(dt)*precent),
IsRewardAreBoolean = FALSE ,
listCategorial=0 ,
listInteger=listInteger)
temp <- cumulativeRegret(res$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
nrow(visitor_reward[(as.integer(nrow(dt)*precent)):nrow(dt),])
length(res$choice)
res$theta_hat
res$theta
dt$meanTS <- 0
#Moyenne
for (i in 1:nrow(dt)) {
dt$meanTS[i] = mean(dt$time_series[[i]])
}
newdt <- dt
newdt$time_series <- NULL
resLinUCB <- LinucbBanditObjectEvaluation(newdt[as.integer(nrow(dt)*precent):nrow(dt),],visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
temp2 <- cumulativeRegret(resLinUCB$linucb_bandit_alloc$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
max(temp)
max(temp2)
max(temp2)-max(temp)
size.tot = 9000
set.seed(4649)                          # this makes the example exactly reproducible
# Time series
alpha_list <- c(1,2,3)
beta_list <- c(0.5,0.1,-0.2)
theta_list <- c(0.8,0.2,0.5)
alpha_list <- c(1,1,1)
beta_list <- c(0.5,0.5,0.5)
theta_list <- c(10,500,1000)
y <- as.data.frame(c(1))
colnames(y) = "ID"
temp=1
for (j in 1:3000){
for (i in 1:length(alpha_list)){
n = sample(1:100,1)
t <- 1:n
ts <- alpha_list[i] + beta_list[i] * t + arima.sim(list(ma = theta_list[i]), n = length(t))
y[temp, "time_series"][[1]] <- list(ts)
y[temp, "cluster"][[1]] <- i
y$ID[temp] = temp
temp = temp +1
}
}
#useless context
x1 = runif(size.tot, min=0, max=10)
dt <-  as.data.frame(cbind(x1,y$time_series,y$cluster))
colnames(dt) <- c("x1","time_series","cluster")
visitor_reward <-  data.frame(rep(0,size.tot),rep(0,size.tot),rep(0,size.tot))
colnames(visitor_reward) = c("K1","K2","K3")
for(i in 1:nrow(dt)) {
if(dt$cluster[i] == 1){
visitor_reward$K1[i] = 3*mean(dt$time_series[[i]])+ rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 2){
visitor_reward$K1[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 3*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
}
if(y$cluster[i] == 3){
visitor_reward$K1[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K2[i] = 1*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
visitor_reward$K3[i] = 3*mean(dt$time_series[[i]]) + rnorm(1,mean = 0, sd =1)
}
}
dt$x1 <- as.numeric(dt$x1)
temp <- dt$cluster
dt$cluster <- NULL
K=ncol(visitor_reward)
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward)
listSerie = c("time_series")
listKCentroids=c(3)
listInteger=c("x1")
precent = 0.7
res <- DBALINUCB_rejection_sampling(dt, visitor_reward,
alpha=1, K=ncol(visitor_reward),
listSerie, listKCentroids ,
learn_size = as.integer(nrow(dt)*precent),
IsRewardAreBoolean = FALSE ,
listCategorial=0 ,
listInteger=listInteger)
temp <- cumulativeRegret(res$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
nrow(visitor_reward[(as.integer(nrow(dt)*precent)):nrow(dt),])
length(res$choice)
res$theta_hat
res$theta
dt$meanTS <- 0
#Moyenne
for (i in 1:nrow(dt)) {
dt$meanTS[i] = mean(dt$time_series[[i]])
}
newdt <- dt
newdt$time_series <- NULL
resLinUCB <- LinucbBanditObjectEvaluation(newdt[as.integer(nrow(dt)*precent):nrow(dt),],visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
temp2 <- cumulativeRegret(resLinUCB$linucb_bandit_alloc$choice,visitor_reward[as.integer(nrow(dt)*precent):nrow(dt),])
max(temp)
max(temp2)
